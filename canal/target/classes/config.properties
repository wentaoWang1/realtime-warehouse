# canal配置
canal.server.ip=node1
canal.server.port=11111
canal.server.destination=example
canal.server.username=canal
canal.server.password=canal
canal.subscribe.filter=itcast_shop.*

# zookeeper配置
zookeeper.server.ip=linux01:2181,linux02:2181,linux03:2181

# kafka配置
# kafka集群地址
kafka.bootstrap_servers_config=linux01:9092,linux02:9092,linux03:9092
# 配置批次发送数据的大小，满足批次大小才会发送数据
kafka.batch_size_config=1024
# 1：表示leader节点写入成功，就返回，假设leader节点写入成功以后没有来得及同步，宕机了，数据会丢失
# 0：异步操作，不管有没有写入成功，都返回，存在丢失数据的可能
# -1：当leader节点写入成功，同时从节点同步成功以后才会返回，可以保证数据的不丢失
kafka.acks=all
# 重试次数 以及 kafka客户端id(随便起名)
kafka.retries=0
kafka.client_id_config=itcast_shop_canal_click
# kafka的key序列化:k-采用string序列化方式 而 value-采用自定义的protobuf序列化方式
kafka.key_serializer_class_config=org.apache.kafka.common.serialization.StringSerializer
# kafka的value序列化，这个序列化方式是需要自定义开发的
kafka.value_serializer_class_config=cn.itcast.canal.protobuf.ProtoBufSerializer
# 数据写入到kafka的哪个topic中
kafka.topic=ods_itcast_shop_mysql